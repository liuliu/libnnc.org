

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>The NNC Tensor Allocation Algorithm &mdash; nnc, a deep learning framework from ccv</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NNC Static Schedule A Graph" href="nnc-schd.html" />
    <link rel="prev" title="Technicals" href="index.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/nnc-logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                a deep learning framework from ccv
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../docs/nnc.html">NNC: Neural Network Collection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../docs/nnc.html#what-s-nnc">What’s NNC?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../docs/nnc.html#tensors-commands-and-streams">1. Tensors, Commands and Streams</a></li>
<li class="toctree-l2"><a class="reference internal" href="../docs/nnc.html#computation-graph">2. Computation Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="../docs/nnc.html#symbolic-graph">3. Symbolic Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="../docs/nnc.html#dynamic-graph">4. Dynamic Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="../docs/nnc.html#common-neural-network-primitives">5. Common Neural Network Primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../docs/nnc.html#supplementary-materials">Supplementary Materials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../docs/nnc.html#toll-free-bridging">Toll-Free Bridging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../docs/nnc.html#automatic-differentiation">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../docs/nnc.html#while-type-sub-graph"><code class="docutils literal notranslate"><span class="pre">while</span></code> Type Sub-Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../docs/nnc.html#case-of-type-sub-graph"><code class="docutils literal notranslate"><span class="pre">case..of</span></code> Type Sub-Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../docs/nnc.html#limits-and-constraints">Limits and Constraints</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Technicals</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">The NNC Tensor Allocation Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tensor-representation">Tensor Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loop-representation">Loop Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-problem-definition">The Problem Definition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-core-algorithm">The Core Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#basic-structure">Basic Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#candidate-selection">Candidate Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#insertion">Insertion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#intuition">Intuition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loop">Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-view-tensor">Multi-view Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loop-with-efficient-tensor-allocation">Loop with Efficient Tensor Allocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sub-computation-graph">Sub-Computation Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nnc-schd.html">NNC Static Schedule A Graph</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nnc-schd.html#stream">Stream</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc-schd.html#static-schedule">Static Schedule</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc-schd.html#while-and-case-of"><code class="docutils literal notranslate"><span class="pre">while</span></code> and <code class="docutils literal notranslate"><span class="pre">case..of</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nnc-dy.html">NNC Dynamic Graph Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nnc-dy.html#naming-the-variable">Naming The Variable</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc-dy.html#tracing-the-operation">Tracing The Operation</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc-dy.html#optimizations">Optimizations</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc-dy.html#interoperability-not-ready">Interoperability (Not Ready)</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc-dy.html#future-optimizations">Future Optimizations</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc-dy.html#some-maybes">Some Maybes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nnc-cnnp.html">NNC Common Neural Network Primitives</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nnc-cnnp.html#model">Model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/level-0.html">Level 0 API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/level-0.html#essentials">Essentials</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/level-1.html">Level 1 API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/level-1.html#tensors">Tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-1.html#commands">Commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-1.html#streams">Streams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/level-2.html">Level 2 API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/level-2.html#essentials">Essentials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-2.html#others">Others</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/level-3.html">Level 3 API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/level-3.html#essentials">Essentials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-3.html#others">Others</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/level-3.5.html">Level 3.5 API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/level_3_5/group__level__3__5__case__of_symbolic_switch.html">Construct “switch” control structure in symbolic graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level_3_5/group__level__3__5__simplify_symbolic_simplify.html">Symbolic graph simplification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level_3_5/group__level__3__5__while_symbolic_while.html">Construct a “while” loop in a symbolic graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-3.5.html#automatic-differentiation">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-3.5.html#while-loop-essentials">While Loop Essentials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-3.5.html#branching">Branching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-3.5.html#gradient-based-optimization">Gradient-based Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-3.5.html#graph-simplification">Graph Simplification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-3.5.html#while-loop-others">While Loop Others</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/level-4.html">Level 4 API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/level-4.html#essentials">Essentials</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/level-5.html">Level 5 API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/level_5/group__level__5_dataframe.html">Dataframe</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level_5/group__level__5_model.html">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/level-5.html#essentials">Essentials</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          
            <a href="../index.html">
          

          
            
            <img src="../_static/nnc-logo.svg" class="logo" alt="Logo"/>
          
          </a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Technicals</a> &raquo;</li>
        
      <li>The NNC Tensor Allocation Algorithm</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tech/nnc-alloc.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="the-nnc-tensor-allocation-algorithm">
<h1>The NNC Tensor Allocation Algorithm<a class="headerlink" href="#the-nnc-tensor-allocation-algorithm" title="Permalink to this headline">¶</a></h1>
<p>Today, most neural network computations are organized as acyclic graph with each node represent a computation with a list of tensors (or multi-dimensional arrays) associated with it. Certain modifications are added to support control structures such as <em>if</em> or <em>do…while</em> loop. Given that for most implementations, they represent the acyclic graph in symbolic form (thus, no computation has been executed and no tensor has been allocated.), an comprehensive and efficient allocation algorithm is desirable and has been shown not only improve space utilization but also the speed (due to data locality).</p>
<p>The NNC tensor allocation algorithm is a take on the problem, it has the following properties:</p>
<ol class="arabic simple">
<li>Treat tensors as a region of memory, enable reuse part of the previously allocated memory;</li>
<li>Support loop structure, thus, if there is a while loop, this allocation algorithm will handle tensor reuse properly, without introduce any extra data transfer operation;</li>
<li>Enable efficient memory reuse when branching.</li>
</ol>
<div class="section" id="tensor-representation">
<h2>Tensor Representation<a class="headerlink" href="#tensor-representation" title="Permalink to this headline">¶</a></h2>
<p>To simplify the description, we will assume tensors are represented as a continuous memory region. A extension of this algorithm allows “alias” representation for a given tensor, thus, pointing to a sub-range of the memory region that tensor represents.</p>
<p>Each tensor is represented symbolically. Static single assignment form is assumed, thus, a tensor can be only assigned as node output once.</p>
</div>
<div class="section" id="loop-representation">
<h2>Loop Representation<a class="headerlink" href="#loop-representation" title="Permalink to this headline">¶</a></h2>
<p>A <em>do…while</em> loop is represented as a sub-graph of existing graph. The sub-graph is represented as one node in the parent graph. All inputs for this while loop are captured as node inputs, and the outputs for this while loop are captured as node outputs. A condition is evaluated at the beginning each round of the loop, and at the end of the loop, inputs are updated with the new outputs based specifications (thus, which input is replaced by which output). Although not proved mathematically, this should enable all types of <em>do…while</em> loop construction.</p>
</div>
<div class="section" id="the-problem-definition">
<h2>The Problem Definition<a class="headerlink" href="#the-problem-definition" title="Permalink to this headline">¶</a></h2>
<p>Before we get into details of the NNC tensor allocation algorithm, let’s get the problem definition straight. Given a graph with above tensors and loops, the problem asks to assign <code class="docutils literal notranslate"><span class="pre">n</span></code> tensors to one memory region buffer such that for each node operation, the input tensors and the output tensors have non-overlap memory regions assigned. each of the <code class="docutils literal notranslate"><span class="pre">n</span></code> tensors also have an offset and size associated to denote from where within the buffer is the tensor memory region. We want to find an arrangement so that the size of the buffer is smallest.</p>
<p>It is easy to see this problem is NP-Complete. Therefore, the challenge is to find a good enough approximation to the optimal solution.</p>
</div>
<div class="section" id="the-core-algorithm">
<h2>The Core Algorithm<a class="headerlink" href="#the-core-algorithm" title="Permalink to this headline">¶</a></h2>
<p>Before stating the core algorithm, there are a few principles we want to follow, and hopefully you will find these principles make sense.</p>
<ol class="arabic simple">
<li>Deterministic and reproducible allocations;</li>
<li>Handle common neural network structures well (ResNet, DenseNet, LTSM etc.);</li>
<li>Handle ties well, and have a well-reasoned way to break the tie.</li>
</ol>
<p>With these in mind, I will first discuss the basic structure of the algorithm, then some alternatives we may have, but why not pursuit. Afterwards, I will discuss one important extension of this algorithm to support <em>do…while</em> loop construction.</p>
</div>
<div class="section" id="basic-structure">
<h2>Basic Structure<a class="headerlink" href="#basic-structure" title="Permalink to this headline">¶</a></h2>
<p>The algorithm consider the above problem as the so-called interference graph, which is widely known for register allocations. In this graph, a node represents a tensor. If there is an edge between two tensors, that means these two tensors has to be allocated to non-overlap memory region.</p>
<p>The interference graph captured the non-overlap nature, however, the partial reuse of tensors is under specified with the interference graph. Thus, we have our first structure to represent the constraints, and now we need our second structure to represent the solution.</p>
<p>The second structure is an acyclic graph with edge weights for our solution. The acyclic graph with edge weights (the allocation graph) has one source node to represent the memory region buffer, a directional edge associated itself with a weight, that represent an allocation of <code class="docutils literal notranslate"><span class="pre">x</span></code> bytes from the edge’s source node to the its destination node. There is one dummy sink node that represents the buffer when its allocation is reclaimed. In this structure, two tensors could be connected only if they don’t interfere with each other, thus, the destination tensor can reuse part of the memory region from the source tensor.</p>
<p>Based on the second structure, an iterative construction of the solution would be to insert tensor nodes into acyclic graph with infinite reservoir of outflow from source node to the sink node until all tensor nodes are inserted. At that point, the size of the buffer would be all weights of the source node’s outgoing edges. Our algorithm now reduced to the candidate tensor selection when forming this graph structure.</p>
</div>
<div class="section" id="candidate-selection">
<h2>Candidate Selection<a class="headerlink" href="#candidate-selection" title="Permalink to this headline">¶</a></h2>
<p>A set of candidates are maintained for the graph insertion. Each candidate is a tuple of tensors (max of 3) that doesn’t interfere with each other. The candidate selection algorithm like this:</p>
<ol class="arabic simple">
<li>Go through all tensors that hasn’t been inserted, find the tensor that has the most number of edges in the interfere graph, if multiple tensors have the same number edges, add them all to the set of candidates;</li>
<li>For each candidate tensor in the set 1, try to find another tensor that doesn’t interfere with it and has larger size than the candidate tensor. Making them a tuple and add to the set of candidates;</li>
<li>Order the set of candidates first by the maximum size of tensors in the tuple, then by the total number of edges on the interference graph of tensors in the tuple.</li>
<li>Go through the ordered set of candidates, try to find an edge on the allocation graph such that the source of the edge and the destination of the edge don’t interfere with any tensors in the tuple and none of the source or the destination of the edge are the dummy source or sink nodes. If such edge is found, we find the candidate.</li>
<li>If none of the candidate can have such edge found in the allocation graph, we select the first candidate.</li>
</ol>
</div>
<div class="section" id="insertion">
<h2>Insertion<a class="headerlink" href="#insertion" title="Permalink to this headline">¶</a></h2>
<p>The selected tuple of tensors then need to be inserted into the allocation graph. The insertion is straight-forward.</p>
<ol class="arabic simple">
<li>During selection, an edge is already picked, if it is not, we make a new edge with weight as the maximum size among the tuple of tensors from dummy source to the dummy sink node.</li>
<li>Tensors in the tuple ordered by its order of access. The order must be available on the computation graph otherwise these tensors will interfere with each other.</li>
<li>The weight of previous edge decreased by the maximum size among the tuple of tensors.</li>
<li>An edge from the previous edge’s source to the first tensor is inserted. The weight on the edge will be the size of the first tensor.</li>
<li>An edge from the first tensor to the second tensor is inserted. If the size of the second tensor is larger than the first tensor, the weight on the new edge will be the size of the first tensor, and another edge is inserted from the source to the second tensor with weight of the difference. Otherwise, the weight on the new edge will be the size of the second tensor.</li>
<li>Similarly, edges from the first tensor, second tensor, or the source will be inserted with respected weights.</li>
<li>Finally, edges from the all tensors to the destination will be inserted with the remaining weights.</li>
</ol>
<p>Repeat above until all tensors are connected in the allocation graph.</p>
</div>
<div class="section" id="intuition">
<h2>Intuition<a class="headerlink" href="#intuition" title="Permalink to this headline">¶</a></h2>
<p>Go with the tensor that has most interference is a common greedy strategy in register allocation. It removes most uncertainty that otherwise needs to branch over.</p>
<p>However, unlike register allocation, in tensor computation graphs, there are less cases that one tensor will span over a large chunk of computations especially in inference stage. Thus, a lot of tensors will have identical number of edges in the interference graph. For these cases, how to break the tie is crucial.</p>
<p>For our allocation algorithm, the allocation size is used as the the tie-breaker. If applying allocation size naively as the second sorting key, in tensor computation graphs, you may still find a lot of cases that you have tie. It is because the tensors that has similar life-span tends to be of the similar usage, thus, has similar dimensionality. For large class of neural networks, we found that by pairing up the tensor has the most interference with the tensor that has larger size (these two have to not interfere with each other), it is more likely for us to reach the trivial solution.</p>
</div>
<div class="section" id="loop">
<h2>Loop<a class="headerlink" href="#loop" title="Permalink to this headline">¶</a></h2>
<p>Tensor allocation with loop has to have a very specific definition of what a loop is. More broadly speaking, the types of control structure in a computation graph to support directly relevant to the allocation algorithm. The loop we specifically concerned are the ones with one conditional statement to exit the loop (traditional while-loop). For NNC tensor allocation algorithm to work, a new construct, called multi-view tensor, need to be introduced. Alternatively, the algorithm introduced here will be applicable to a specific loop that contains multiple conditional exits and phi function.</p>
<p>If you accept that certain data transfer is required for loop to work, the loop handling for tensor allocation algorithm is trivial. <strong>A loop can be considered as a sub-computation graph</strong>, and the same allocation algorithm can be applied to the sub-computation graph. When reached the end of the graph and we need to loop over again, data can then be transferred to the parameters.</p>
<p>For example, if you have:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="k">while</span> <span class="p">(</span><span class="n">c</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">)</span> <span class="p">{</span> <span class="o">//</span> <span class="n">c</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">loop</span> <span class="n">counter</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">Convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="p">})(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">y</span><span class="p">)</span> <span class="o">//</span> <span class="n">This</span> <span class="n">syntax</span> <span class="n">means</span> <span class="ow">in</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">loop</span><span class="p">,</span> <span class="n">x</span> <span class="n">will</span> <span class="n">contain</span> <span class="n">the</span> <span class="n">content</span> <span class="n">of</span> <span class="n">y</span><span class="p">,</span> <span class="n">you</span> <span class="n">can</span> <span class="n">think</span> <span class="n">of</span> <span class="n">this</span> <span class="k">as</span> <span class="n">x</span> <span class="o">=</span> <span class="n">Convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">but</span> <span class="n">such</span> <span class="n">representation</span> <span class="ow">is</span> <span class="n">forbidden</span> <span class="ow">in</span> <span class="n">static</span> <span class="n">single</span> <span class="n">assignment</span> <span class="n">form</span><span class="o">.</span>
</pre></div>
</div>
<p>The tensor allocation algorithm is trivial is we accept that we need to transfer data from <code class="docutils literal notranslate"><span class="pre">y</span></code> to <code class="docutils literal notranslate"><span class="pre">x</span></code> every time. This section however, we will discuss how to completely eliminate such data transfer with a novel and generic tensor allocation scheme.</p>
</div>
<div class="section" id="multi-view-tensor">
<h2>Multi-view Tensor<a class="headerlink" href="#multi-view-tensor" title="Permalink to this headline">¶</a></h2>
<p>This is a special tensor that with nested structure. For a leaf multi-view tensor, it can point to multiple memory regions based on the loop counter. Particularly, a multi-view tensor can be configured with a repeat length. Its pointer will be updated prior to the actual computation each round the the correct memory region: <code class="docutils literal notranslate"><span class="pre">ptr</span> <span class="pre">=</span> <span class="pre">ptrs[loop_counter</span> <span class="pre">%</span> <span class="pre">repeat_length]</span></code>. There are some complications such as the support for two types of multi-view tensors. Type I will be the one described above. Type II will have a special memory region that only used when <code class="docutils literal notranslate"><span class="pre">loop_counter</span> <span class="pre">==</span> <span class="pre">0</span></code>.</p>
<p>A multi-view tensor can not only points to memory regions, but to a set of other multi-view tensors, following the same semantics, thus, the nested structure.</p>
</div>
<div class="section" id="loop-with-efficient-tensor-allocation">
<h2>Loop with Efficient Tensor Allocation<a class="headerlink" href="#loop-with-efficient-tensor-allocation" title="Permalink to this headline">¶</a></h2>
<p>Above are all the constructs we need to implement efficient tensor allocation algorithm (the efficient here means no data transfer required).</p>
<p>For each parameter, we first identify whether co-allocating them to the same memory region is sufficient. In some cases, they are, thus, we can simply do that and then apply our tensor allocation algorithm to the sub-computation graph.</p>
<p>However, in some cases (like the superficial case we made above), it is not possible. For these, we need to <em>unroll</em> the loop.</p>
<p>For example, unrolled above loop will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">while</span> <span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">Convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="n">exit</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">Convolution</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
  <span class="n">c</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span>
<span class="p">}(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">y</span><span class="p">,</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<p>One extra conditional exit added to make the loop syntactically equivalent to the one we had before.</p>
<p>When a loop unrolled as above, for the particular case, we can see that now <code class="docutils literal notranslate"><span class="pre">y</span></code> can be co-allocated with <code class="docutils literal notranslate"><span class="pre">x</span></code> (They are not interfere with each other).</p>
<p>It can be proved that any loop can be unrolled into a form that the parameters can be co-allocated. The exercise will be left to readers on how to use this to tackle something like <code class="docutils literal notranslate"><span class="pre">x[c]</span> <span class="pre">=</span> <span class="pre">Convolution(x[c</span> <span class="pre">-</span> <span class="pre">4],</span> <span class="pre">w,</span> <span class="pre">b)</span></code> which requires to access variable from several loops before.</p>
<p>Once a loop can co-allocate all its parameters after unrolling, we can apply the tensor allocation algorithm on the unrolled computation graph.</p>
<p>The allocation on the unrolled computation graph then can be used to create the multi-view tensors. Now, the repeat length on the multi-view tensors correspond to how many times we unrolled the loop. Each memory region will be pointing to corresponding tensor on the unrolled computation graph as well.</p>
</div>
<div class="section" id="sub-computation-graph">
<h2>Sub-Computation Graph<a class="headerlink" href="#sub-computation-graph" title="Permalink to this headline">¶</a></h2>
<p>Sub-computation graph’s tensor allocation generated number of buffers and each buffer size. These will be used as regular tensor in the parent computation graph. The whole allocation algorithm then becomes recursive.</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>I believe the above algorithm is the first to address the tensor allocation problem with partial memory reuse and loop efficiency in mind. This algorithm is also presented as an extensible framework that can be considered in the future to support more control structures.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nnc-schd.html" class="btn btn-neutral float-right" title="NNC Static Schedule A Graph" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="Technicals" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, libnnc.org.
      
        <span class="build">
          <a href="https://github.com/liuliu/ccv">View the project on GitHub</a>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'a deep learning framework from ccv',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>