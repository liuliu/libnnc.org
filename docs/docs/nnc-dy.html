

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>NNC Dynamic Graph Execution &mdash; nnc a deep learning framework from ccv documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NNC Common Neural Network Primitives" href="nnc-cnnp.html" />
    <link rel="prev" title="The NNC Tensor Allocation Algorithm" href="nnc-alloc.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> nnc
          

          
          </a>

          
            
            
              <div class="version">
                a deep learning framework from ccv
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="nnc.html">NNC: Neural Network Collection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nnc.html#what-s-nnc">What’s NNC?</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc.html#tensors-commands-and-streams">1. Tensors, Commands and Streams</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc.html#computation-graph">2. Computation Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc.html#symbolic-graph">3. Symbolic Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc.html#dynamic-graph">4. Dynamic Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc.html#common-neural-network-primitives">5. Common Neural Network Primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc.html#supplementary-materials">Supplementary Materials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nnc.html#toll-free-bridging">Toll-Free Bridging</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc.html#automatic-differentiation">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc.html#while-type-sub-graph"><code class="docutils literal notranslate"><span class="pre">while</span></code> Type Sub-Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc.html#case-of-type-sub-graph"><code class="docutils literal notranslate"><span class="pre">case..of</span></code> Type Sub-Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="nnc.html#limits-and-constraints">Limits and Constraints</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nnc-alloc.html">The NNC Tensor Allocation Algorithm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#tensor-representation">Tensor Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#loop-representation">Loop Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#the-problem-definition">The Problem Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#the-core-algorithm">The Core Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#basic-structure">Basic Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#candidate-selection">Candidate Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#insertion">Insertion</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#intuition">Intuition</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#loop">Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#multi-view-tensor">Multi-view Tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#loop-with-efficient-tensor-allocation">Loop with Efficient Tensor Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#sub-computation-graph">Sub-Computation Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnc-alloc.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">NNC Dynamic Graph Execution</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#naming-the-variable">Naming The Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tracing-the-operation">Tracing The Operation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizations-part-1">Optimizations Part 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizations-part-2-not-ready">Optimizations Part 2 (Not Ready)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interoperability-not-ready">Interoperability (Not Ready)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#some-maybes">Some Maybes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nnc-cnnp.html">NNC Common Neural Network Primitives</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nnc-cnnp.html#model">Model</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nnc</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>NNC Dynamic Graph Execution</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/docs/nnc-dy.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nnc-dynamic-graph-execution">
<h1>NNC Dynamic Graph Execution<a class="headerlink" href="#nnc-dynamic-graph-execution" title="Permalink to this headline">¶</a></h1>
<p>Frameworks such as <strong>PyTorch</strong> or <strong>TensorFlow Eager</strong> nowadays have dynamic graph support, which is a fancy word to describe when a computation is carried out while constructing the computation graph.</p>
<p>If <strong>dynamic graph execution</strong> is just about executing a command when issuing it, this is not interesting. <strong>Dynamic graph execution</strong> by these frameworks also supports <em>automatic differentiation</em>. A good <strong>dynamic graph execution</strong> framework such as <strong>PyTorch</strong> enables easier debugging, more intuitive coding thus quicker experimentation cycle.</p>
<p>That has been said, there are a few drawbacks when you support <strong>dynamic graph execution</strong> naively.</p>
<ol class="arabic simple">
<li>Limited optimization opportunities. With <strong>dynamic graph execution</strong>, the framework lacks the foresight, makes optimizations such as <em>common sub-expression elimination</em> or <em>data layout optimization</em> hard to implement;</li>
<li>Unbounded memory usage. Since a <strong>dynamic graph execution</strong> engine needs to be able to differentiate arbitrary variables within the framework, a Wengert list (a tape) has to be kept. In many situations, to trim that list requires user attention otherwise the memory usage will continue to grow.</li>
</ol>
<p>To work-around 1., mixing <strong>static graph execution</strong> with <strong>dynamic graph execution</strong> is desirable. However, that imposes its own set of problems: when a <strong>static graph</strong> contains a <strong>dynamic graph</strong>, and if the <strong>static graph</strong> contains a loop structure, the tape for the <strong>static graph</strong> need to cross into the <strong>dynamic graph</strong> to continue work. When a <strong>dynamic graph</strong> contains a <strong>static graph</strong>, the Wengert list (the tape) of the <strong>dynamic graph</strong> need to not only store the tensors, but also the <strong>static graph</strong> as a whole.</p>
<p>NNC’s <strong>dynamic graph execution</strong> design will attempt to address above problems with reasonable compromises. It borrows some good ideas from 10 years ago when I first started to implement ccv.</p>
<div class="section" id="naming-the-variable">
<h2>Naming The Variable<a class="headerlink" href="#naming-the-variable" title="Permalink to this headline">¶</a></h2>
<p>Like in most frameworks, <strong>dynamic graph execution</strong> in NNC operates at variables. <strong>Dynamic graph</strong> executes command on a set of input variables, writes the result to a set of output variables. Variables can be inspected anytime with <code class="docutils literal notranslate"><span class="pre">ccv_nnc_tensor_from_variable</span></code>. The underlying tensor may not be allocated when the variable is created. <code class="docutils literal notranslate"><span class="pre">ccv_nnc_tensor_variable_t</span></code> is an opaque structure and its inner work shouldn’t be of an interest to users.</p>
</div>
<div class="section" id="tracing-the-operation">
<h2>Tracing The Operation<a class="headerlink" href="#tracing-the-operation" title="Permalink to this headline">¶</a></h2>
<p>Frameworks such as <strong>PyTorch</strong> or <strong>TensorFlow Eager</strong> use the tape to record which operations are executed, and record the inputs / outputs along the way. <em>automatic differentiation</em> was implemented (its reverse mode) by walking back on the tape. This is simple to implement, and easier to support higher order gradients (by record another tape while walking back on the existing tape). This also makes optimizations on the <em>automatic differentiation</em> pass difficult because no data dependencies are specified. It is definitely possible to infer the data dependencies from the tape, and then employ optimizations or automatic parallelization. For mature framework such as <strong>TensorFlow</strong>, that kind of work is to reimplement some of the fundamental pieces of the software.</p>
<p>NNC uses its <strong>symbolic graph</strong> (Level-3 APIs) to trace the operation. When a command executed on a <strong>dynamic graph</strong>, we can figure out data dependencies with input variables (each input variable has a unique tensor symbol assigned). Even though the variables in the <strong>dynamic graph</strong> don’t follow the <em>static single assignment</em> (SSA) rule, the underlying tensors and tensor symbols do. Thus, through the normal execution of the <strong>dynamic graph</strong>, we have formed a full <strong>symbolic graph</strong> for later computation.</p>
<p>Upon <em>automatic differentiation</em>, no tape is used (or, the <strong>symbolic graph</strong> serves as an advanced tape). We simply leverage the ahead of time <em>automatic differentiation</em> system implemented in <strong>symbolic graph</strong> to optimize, compile and schedule the actual computation. That means any optimization techniques we implemented on Level-2 or Level-3 APIs will be available to <strong>dynamic graph</strong> as well.</p>
</div>
<div class="section" id="optimizations-part-1">
<h2>Optimizations Part 1<a class="headerlink" href="#optimizations-part-1" title="Permalink to this headline">¶</a></h2>
<p>In <strong>PyTorch</strong>, there is a need to <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> such that the framework knows which variable should be discarded to save memory. If it is not done carefully, the memory usage can grow unbounded. <strong>Dynamic graph</strong> here provides <code class="docutils literal notranslate"><span class="pre">ccv_nnc_tensor_variable_free</span></code> where when a tensor variable is freed, we will release its memory when it is safe. This method meant to hook up with object finalization methods in host languages (C++’s destructor, Objective-C’s <code class="docutils literal notranslate"><span class="pre">dealloc</span></code>, <code class="docutils literal notranslate"><span class="pre">deinit</span></code> in Swift, <code class="docutils literal notranslate"><span class="pre">finalize</span></code> in Java, <code class="docutils literal notranslate"><span class="pre">tp_dealloc</span></code> in Python).</p>
</div>
<div class="section" id="optimizations-part-2-not-ready">
<h2>Optimizations Part 2 (Not Ready)<a class="headerlink" href="#optimizations-part-2-not-ready" title="Permalink to this headline">¶</a></h2>
<p>At this point, <strong>dynamic graph</strong> looks suspiciously like just another function dispatching mechanism. Ten years ago, when I started ccv, one of the motivation is to implement a function memorization technique, at that time, it is called <em>cached image processing</em> to workaround issues that in traditional computer vision pipeline, low level feature extraction passes often shared between different components (face detector, motion tracker etc.). In <strong>symbolic graph</strong>, this is trivially implemented as <em>common sub-expression elimination</em> (CSE). CSE cannot be implemented in <strong>dynamic graph</strong> because it cannot look ahead. However, the same memorization technique can be used to avoid duplicate computations.</p>
<p>Because <strong>symbolic graph</strong> formed from <strong>dynamic graph execution</strong> contains the proper data dependencies, memory reduction techniques such as automatic binomial checkpointing can be implemented with a change of cache eviction policy. If we implemented binomial checkpointing in <strong>symbolic graph</strong> as one optimization pass, we can also leverage that upon <em>automatic differentiation</em> in <strong>dynamic graph</strong>. The flexibility of sharing the same underlying infrastructure is very satisfying.</p>
</div>
<div class="section" id="interoperability-not-ready">
<h2>Interoperability (Not Ready)<a class="headerlink" href="#interoperability-not-ready" title="Permalink to this headline">¶</a></h2>
<p>There are some sticky issues with interoperability between <strong>static graph</strong> (the <strong>symbolic graph</strong> we formed by hand) with <strong>dynamic graph</strong>. The way they interoperate is through <code class="docutils literal notranslate"><span class="pre">CCV_NNC_CUSTOM_FORWARD</span></code> / <code class="docutils literal notranslate"><span class="pre">CCV_NNC_CUSTOM_BACKWARD</span></code> functions. When a <strong>static graph</strong> includes a <strong>dynamic graph</strong>, its tape needs to book-keeping for the <strong>dynamic graph</strong>. When a <strong>dynamic graph</strong> includes a <strong>static graph</strong>, it also needs to create a tape at that point for the execution. All these implies significant changes for the <code class="docutils literal notranslate"><span class="pre">ccv_nnc_tensor_tape_t</span></code> implementation to accommodate these new requirements.</p>
</div>
<div class="section" id="some-maybes">
<h2>Some Maybes<a class="headerlink" href="#some-maybes" title="Permalink to this headline">¶</a></h2>
<p>One of the major reason (or the reason) to use <strong>dynamic graph</strong> is its unparalleled debuggability. You can inspect tensors as you go in the code. However, this ability can be retained if the execution is separated from the <strong>dynamic graph</strong> forming. Your code can go a long way by forming computations and the underlying execution could be asynchronous. The synchronization happens only when you inspect these tensors to either debug, or practically, determine the control flow. This also offers limited look ahead ability to <strong>dynamic graph</strong> that enables more shared optimizations from Level-3 APIs. Implementing this is complicated. Synchronization point can easily turned into deadlock point, and the inter-play of <strong>static graph</strong> inside a <strong>dynamic graph</strong> inside a <strong>static graph</strong> could be more delicate. In a world where we modify languages to extract <strong>static graph</strong> (Swift for TensorFlow), the reason to have this kind of sophisticated <strong>dynamic graph</strong> implementation may be mooted.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nnc-cnnp.html" class="btn btn-neutral float-right" title="NNC Common Neural Network Primitives" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="nnc-alloc.html" class="btn btn-neutral" title="The NNC Tensor Allocation Algorithm" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Liu Liu.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'a deep learning framework from ccv',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>